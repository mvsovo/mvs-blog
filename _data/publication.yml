publication:
  - year: '2023'
    pubs:
      - title: What makes virtual intimacy...intimate? Understanding the Phenomenon and Practice of Computer-Mediated Paid Companionship
        image: satellite-explainability-cscw.jpg
        authors:
          - Weijun Li
          - Shi Chen
          - Lingyun Sun
          - Changyuan Yang
        id: cscw2023_vlover_paper
        venue: CSCW 2023
        venue_full: ''
        abstract:
        note: accepted with minor revision
        category: 
          - "AI / NLP"
          - "CSCW"
        featured: true
  - year: '2021'
    pubs:
      - title: PTeacher, a Computer-Aided Personalized Pronunciation Training System with Exaggerated Audio-Visual Corrective Feedback
        image: omhc.jpg
        authors:
          - Yaohua Bu*
          - Tianyi Ma*
          - Weijun Li
          - Hang Zhou
          - Jia Jia
          - Kaiyuan Xu
          - Dachuan Shi
          - Haozhe Wu
          - Zhihan Yang
          - Kun Li
          - Zhiyong Wu
          - Yuanchun Shi
          - Xiaobo Lu
          - Ziwei Liu

        id: chi2021_capt_paper
        video: AeDKn5DwVfI

        venue: CHI 2021
        venue_full: 2021 CHI Conference on Human Factors in Computing Systems
        abstract: Second language (L2) English learners often find it difficult to improve their pronunciations due to the lack of expressive and personalized corrective feedback. In this paper, we present Pronunciation Teacher (PTeacher), a Computer-Aided Pronunciation Training (CAPT) system that provides personalized exaggerated audio-visual corrective feedback for mispronunciations. Though the effectiveness of exaggerated feedback has been demonstrated, it is still unclear how to define the appropriate degrees of exaggeration when interacting with individual learners. To fill in this gap, we interview 100 L2 English learners and 22 professional native teachers to understand their needs and experiences. Three critical metrics are proposed for both learners and teachers to identify the best exaggeration levels in both audio and visual modalities. Additionally, we incorporate the personalized dynamic feedback mechanism given the English proficiency of learners. Based on the obtained insights, a comprehensive interactive pronunciation training course is designed to help L2 learners rectify mispronunciations in a more perceptible, understandable, and discriminative manner. Extensive user studies demonstrate that our system significantly promotes the learners' learning efficiency.
        bibtex: |-
          @inproceedings{10.1145/3411764.3445490,
              author = {Bu, Yaohua and Ma, Tianyi and Li, Weijun and Zhou, Hang and Jia, Jia and Chen, Shengqi and Xu, Kaiyuan and Shi, Dachuan and Wu, Haozhe and Yang, Zhihan and Li, Kun and Wu, Zhiyong and Shi, Yuanchun and Lu, Xiaobo and Liu, Ziwei},
              title = {PTeacher: A Computer-Aided Personalized Pronunciation Training System with Exaggerated Audio-Visual Corrective Feedback},
              year = {2021},
              isbn = {9781450380966},
              publisher = {Association for Computing Machinery},
              address = {New York, NY, USA},
              url = {https://doi.org/10.1145/3411764.3445490},
              doi = {10.1145/3411764.3445490},
              abstract = {Second language (L2) English learners often find it difficult to improve their pronunciations due to the lack of expressive and personalized corrective feedback. In this paper, we present Pronunciation Teacher&nbsp;(PTeacher), a Computer-Aided Pronunciation Training (CAPT) system that provides personalized exaggerated audio-visual corrective feedback for mispronunciations. Though the effectiveness of exaggerated feedback has been demonstrated, it is still unclear how to define the appropriate degrees of exaggeration when interacting with individual learners. To fill in this gap, we interview 100 L2 English learners and 22 professional native teachers to understand their needs and experiences. Three critical metrics are proposed for both learners and teachers to identify the best exaggeration levels in both audio and visual modalities. Additionally, we incorporate the personalized dynamic feedback mechanism given the English proficiency of learners. Based on the obtained insights, a comprehensive interactive pronunciation training course is designed to help L2 learners rectify mispronunciations in a more perceptible, understandable, and discriminative manner. Extensive user studies demonstrate that our system significantly promotes the learners’ learning efficiency.},
              booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
              articleno = {676},
              numpages = {14},
              keywords = {Language Learning, Exaggerated feedback, Computer-Aided Pronunciation Training System, Audio-Visual Corrective Feedback},
              location = {Yokohama, Japan},
              series = {CHI '21}
              }
        category: 
          - "AI / NLP"
          - "Healthcare"
          - "CSCW"
        featured: true
      
  - year: '2021'
    pubs:
      - title: Visual-speech Synthesis of Exaggerated Corrective Feedback
        image: omhc-flow.jpg
        authors:
          - Yaohua Bu*
          - Weijun Li*
          - Tianyi Ma
          - Shengqi Chen
          - Jia Jia
          - Kun Li
          - Xiaobo Lu
        id: mm2020_ecf_demo
        venue: MM 2020
        venue_full: Proceedings of the 28th ACM International Conference on Multimedia
        abstract: To provide more discriminative feedback for the second language (L2) learners to better identify their mispronunciation, we propose a method for exaggerated visual-speech feedback in computer-assisted pronunciation training (CAPT). The speech exaggeration is realized by an emphatic speech generation neural network based on Tacotron, while the visual exaggeration is accomplished by ADC Viseme Blending, namely increasing Amplitude of movement, extending the phone's Duration and enhancing the color Contrast. User studies show that exaggerated feedback outperforms non-exaggerated version on helping learners with pronunciation identification and pronunciation improvement.
        bibtex: |-
          @inproceedings{10.1145/3394171.3414444,
            author = {Bu, Yaohua and Li, Weijun and Ma, Tianyi and Chen, Shengqi and Jia, Jia and Li, Kun and Lu, Xiaobo},
            title = {Visual-Speech Synthesis of Exaggerated Corrective Feedback},
            year = {2020},
            isbn = {9781450379885},
            publisher = {Association for Computing Machinery},
            address = {New York, NY, USA},
            url = {https://doi.org/10.1145/3394171.3414444},
            doi = {10.1145/3394171.3414444},
            abstract = {To provide more discriminative feedback for the second language (L2) learners to better identify their mispronunciation, we propose a method for exaggerated visual-speech feedback in computer-assisted pronunciation training (CAPT). The speech exaggeration is realized by an emphatic speech generation neural network based on Tacotron, while the visual exaggeration is accomplished by ADC Viseme Blending, namely increasing Amplitude of movement, extending the phone's Duration and enhancing the color Contrast. User studies show that exaggerated feedback outperforms non-exaggerated version on helping learners with pronunciation identification and pronunciation improvement.},
            booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
            pages = {4521–4523},
            numpages = {3},
            keywords = {emphatic speech synthesis, corrective feedback, visual-speech exaggeration, pronunciation learning},
            location = {Seattle, WA, USA},
            series = {MM '20}
            }
        note: Demo
        category: 
          - "AI / NLP"
          - "Healthcare"
          - "CSCW"
      - title: "An Integrated Method to Build Read-Aiding E-Books Based on Text Mining and Interactive Aesthetics"
        image: amslertouch.jpg
        id: jcdl2020_ebook_poster
        video: ohL-t6EBHT8
        authors:
          - Weijun Li
          - Fukai Yang
          - Ruibing Jia
          - Renmin Li
          - Minghao Yin
        note: Poster
        venue: JCDL 2020
        venue_full: Proceedings of the ACM/IEEE Joint Conference on Digital Libraries
        category: JCDL 2020
        bibtex: |-
          @inproceedings{10.1145/3383583.3398587,
            author = {Li, Weijun and Yang, Fukai and Jia, Ruibing and Li, Renmin and Yin, Minghao},
            title = {An Integrated Method to Build Read-Aiding E-Books Based on Text Mining and Interactive Aesthetics},
            year = {2020},
            isbn = {9781450375856},
            publisher = {Association for Computing Machinery},
            address = {New York, NY, USA},
            url = {https://doi.org/10.1145/3383583.3398587},
            doi = {10.1145/3383583.3398587},
            booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
            pages = {509–510},
            numpages = {2},
            keywords = {text mining, interactive e-book, information visualization},
            location = {Virtual Event, China},
            series = {JCDL '20}
            }
        abstract: 

      